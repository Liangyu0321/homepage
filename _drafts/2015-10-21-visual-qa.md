---
layout: post
title: "Deep Learning for Visual Question Answering"
excerpt: Answering questions about images using Neural Networks.
modified:
category: deeplearning
tags: vision, nlp, deeplearning
image:
  feature: 
  credit: 
  creditlink: 
comments: true
share: 
---
{% include mathjs %}
The source code for the neural models covered in this blog post is written in Python and [Keras](http://keras.io), and is available on [Github](http://github.com/avisingh599/visual-qa). 

An year or so ago, a chatbot named [Eugene Gustman](https://en.wikipedia.org/wiki/Eugene_Goostman) made it to the mainstream news, after having been reported as the first computer program to have passed the famed [Turing Test](http://wiki) in an event organized at the University of Reading[^1]. While the organizers hailed it as a historical achievement, most of the scientific community wasn't impressed. For me, the takeaway from this fiasco was that we need a better test for evaluating the current state of AI.

In the last couple of years, a number of papers[^2] have suggested that the task of Visual Question Answering (VQA, for short) can be used as an alternative Turing Test. This task involves answering a question (or a series of questions) about an image. An example is shown below:


### Image from visualqa.org
![Visual QA](/images/vqa/challenge.png)

Solving the above problem requires the AI system to solve a number of sub-problems in Natural Language Processing and Computer Vision, in addition to being able to perform some kind of "common-sense" reasoning. It needs to localize the subject being referenced (the woman's face, and more specifically the region around her lips), needs to detect objects (the banana), and should also have some common-sense knowledge that the word mustache is often used to refer to markings on the face that are not actually mustaches (like milk mustaches). Since the problem cuts through two two very different modalities (vision and text), and requires high-level understanding instead of simply solving lower level tasks, it appears to be an ideal candidate for a true Turing Test. And apart from quenching scientific curiosity, this problem also has real world applications, like helping the visually impaired[^3]. 

A few days ago, the [Visual QA Challenge](http://visualqa.org/challenge.html) was launched, and along with it came a large dataset (~750K questions on ~250K images). After the [MS COCO Image Captioning Challenge](http://mscoco.org/dataset/#captions-challenge2015) sparked a lot of interest in problem of image captioning[^4], the time seems ripe to move onto a much harder problem at the intersection of NLP and Vision. 

This post will present ways to model this problem using Neural Networks, exploring both Feedforward Neural Networks, and the much more exciting **Recurrent Neural Networks** (LSTMs, to be specific). If you do not know much about Neural Networks, then I encourage you to check these two awesome blogs: [Colah's Blog](https://colah.github.io) and [Karpathy's Blog](https://karpathy.github.io). Specifically, check out the posts on Recurrent Neural Nets and LSTMs. The models in this post take inspiration from [this ICCV 2015 paper](http://deviparikh), and [this NIPS 2015 paper](http://utoronto). 


## Generating Answers
An important aspect of solving this problem is to have a system that can generate new answers. While most of the answers in the VQA dataset are short (1-3 words), we would still like to a have a system that can generate arbitrarily long answers, keeping up with our spirit of the Turing test. We can perhaps take inspiration from papers on [Sequence to Sequence Learning](http//arxiv.org/), that solve a similar problem when generating translations of arbitrary length using neural networks. However, for the purpose of this blog post, we will ignore this aspect of the problem. We will select the 1000 most frequent answers in the VQA dataset, and solve the problem in a multi-class classification setting. These top 1000 answers cover over 80% of the answers in the VQA training/validation set, so we can still expect to get reasonable results. 

## The Feedforward Neural Model
To warm up, let's first try to model the problem using a [MultiLayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron). An MLP is a simple feedforward neural net that maps a feature vector (of fixed length) to an appropriate output. In our problem, this output will be a probability distribution over the set of possible answers. We will be using [Keras](http://keras.io), an awesome deep learning library based on [Theano](http://deeplearning.net/software/theano/), and written in Python. Setting up Keras on your own system is fairly easy, just have a look at their [readme](https://github.com/fchollet/keras).

In order to use the MLP model, we need to map all our input questions and images to a feature vector of fixed length. We do this as follows: 

1. For question, we compute [word vectors](https://code.google.com/p/word2vec/) for all the words in the question, and sum them up. The length of this feature vector will be same as the length of a single word vector, and the embeddings that we use have a length of `300`. 
2. For image, we pass it through a Deep Convolutional Neural Network, the [VGG Architecture](http://arxiv.org/abs/1409.1556), and remove extract the activation from the second last layer. This length of this feature vector is `4096`. 

Once we have generated the feature vectors, all we need to do now is to define a model in Keras, set up a cost function and an optimizer, and we're good to go. 

The following Keras code defines a multi-layer perceptron with two hidden layers, and dropout layers int he middle for regularization. The final layer is a softmax layer, and is responsible for generating the final outputs. We use the `categorical_crossentropy` loss since it is a multi-class classification problem, and 
{% highlight python %}
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation

img_dim = 4096 #top layer of the VGG net
word_vec_dim = 300 #dimension of pre-trained word vectors
numHiddenUnits = 1024 #number of hidden units, a hyperparameter

model = Sequential()
model.add(Dense(numHiddenUnits, input_dim=img_dim+word_vec_dim, 
          init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(numHiddenUnits, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes, init='uniform'))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adagrad')

{% endhighlight %}

Have a look at the [entire python script]() to see the code for generating the features and actually training the network. It only operates on parts of the dataset at a time so as to save memory, and computes features on the fly. The code can be made much faster if you have a large RAM, since you can load all the pre-computed features at once, and train on the entire dataset. On the flip side, if you find that this code itself is too much for your RAM, try reducing the `batchSize` variable. 

## The Recurrent Neural Model

{% highlight python %}
image_model = Sequential()
image_model.add(Dense(featureDim, input_dim=4096, 
                init='uniform', activation='linear'))
image_model.add(RepeatVector(1))
image_model.add(LSTM(output_dim = 512, return_sequences=True, 
                input_shape=(1, featureDim)))

language_model = Sequential()
language_model.add(LSTM(output_dim = 512, return_sequences=True, 
                  input_shape=(maxlen, featureDim)))

model = Sequential()
model.add(Merge([image_model, language_model], mode='concat', 
          concat_axis=1))
print model.output_shape
model.add(LSTM(512, return_sequences=False))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
print 'Compilation done'
{% endhighlight %}




---
[^1]: http://www.bbc.com/news/technology-27762088
[^2]: Geman and Geman, MPI paper
[^3]: VizWiz iOS app
[^4]: Add the bazillion image caption papers
